{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c4e254b-5530-4cc8-b867-e1c4e3dba25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: F:\\RNN based Object detection and Anomaly Classification surveillance System\\Hypersonic Boundary Layer Transition Prediction\\generated_dataset_M4.5-12_cone_trajectory_10000.csv\n",
      "Dataset shape: (10000, 27)\n",
      "Warning: some FEATURE_COLS missing from CSV. Those will be imputed or computed if possible: ['Re_per_m', 'Re_x', 'Knudsen', 'delta_99_m', 'momentum_theta_m', 'Re_theta']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing required target/physics column in CSV: momentum_theta_m (expected). Please check your CSV.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 330\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved test predictions to\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUT_PRED_CSV)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 330\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[11], line 203\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m [TARGET_COL_TRANS, TARGET_COL_THETA, TARGET_COL_DELTA]:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required target/physics column in CSV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (expected). Please check your CSV.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# If physics columns not present separately, use the same computed columns as physics values\u001b[39;00m\n\u001b[0;32m    206\u001b[0m phys_theta_col \u001b[38;5;241m=\u001b[39m PHYS_THETA_COL \u001b[38;5;28;01mif\u001b[39;00m PHYS_THETA_COL \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m TARGET_COL_THETA\n",
      "\u001b[1;31mValueError\u001b[0m: Missing required target/physics column in CSV: momentum_theta_m (expected). Please check your CSV."
     ]
    }
   ],
   "source": [
    "# train_pinn.py\n",
    "\"\"\"\n",
    "Physics-Informed Neural Network (PINN) - adapted to use the same dataset,\n",
    "features and physics-derived inputs as the RF pipeline you provided.\n",
    "\n",
    "Key points:\n",
    " - CSV_PATH updated to the dataset you provided.\n",
    " - Feature list and physics columns match the \"enhanced_features\" used earlier.\n",
    " - Targets mapped to the dataset columns: 'transition_detected',\n",
    "   'momentum_theta_m' (theta), 'delta_99_m' (delta).\n",
    " - If separate physics/blasius columns are absent, the script uses the\n",
    "   same computed momentum_theta_m and delta_99_m as the physics values\n",
    "   (so the physics regularizer remains active).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# TF / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIG - updated to your dataset & inputs\n",
    "# ------------------------------\n",
    "CSV_PATH = r\"F:\\RNN based Object detection and Anomaly Classification surveillance System\\Hypersonic Boundary Layer Transition Prediction\\generated_dataset_M4.5-12_cone_trajectory_10000.csv\"\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "SCALER_PATH = os.path.join(MODEL_DIR, \"scaler.joblib\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"pinn_model.h5\")\n",
    "OUT_PRED_CSV = os.path.join(MODEL_DIR, \"test_predictions_conform.csv\")\n",
    "\n",
    "# Training hyperparameters (kept same as before)\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.15\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "LAMBDA_PHYS = 0.5\n",
    "LOSS_WEIGHTS = [1.0, 1.0, 1.0]\n",
    "\n",
    "# Feature set: same enhanced_features used by the RF pipeline\n",
    "FEATURE_COLS = [\n",
    "    'init_Mach','init_altitude_m','init_velocity_m_s','Tw_over_Tinf_init',\n",
    "    'cone_half_angle_deg','nose_radius_m','x_sensor_m','mass_kg','A_ref_m2','roughness_m',\n",
    "    'Re_per_m','Re_x','Knudsen','delta_99_m','momentum_theta_m','Re_theta'\n",
    "]\n",
    "\n",
    "# Targets (mapped to your dataset)\n",
    "TARGET_COL_TRANS = 'transition_detected'    # binary label in your CSV\n",
    "TARGET_COL_THETA = 'momentum_theta_m'       # used as theta target\n",
    "TARGET_COL_DELTA = 'delta_99_m'             # used as delta target\n",
    "\n",
    "# If you have separate physics/blasius columns, you may set them here; otherwise\n",
    "# we'll fall back to using the dataset's computed momentum_theta_m and delta_99_m\n",
    "PHYS_THETA_COL = 'momentum_theta_m'    # physics theta (Blasius / computed)\n",
    "PHYS_DELTA_COL = 'delta_99_m'          # physics delta (Blasius / computed)\n",
    "\n",
    "# ------------------------------\n",
    "# constants (same as earlier code)\n",
    "# ------------------------------\n",
    "R = 287.058\n",
    "gamma = 1.4\n",
    "mu0 = 1.716e-5\n",
    "T0_ref = 273.15\n",
    "S_sutherland = 110.4\n",
    "\n",
    "DEFAULTS = {\n",
    "    'cone_half_angle_deg': 10.0,\n",
    "    'nose_radius_m': 0.01,\n",
    "    'x_sensor_m': 1.0,\n",
    "    'T_wall_K': 300.0,\n",
    "    'mass_kg': 500.0,\n",
    "    'A_ref_m2': np.pi * 0.01**2,\n",
    "    'roughness_m': 1e-6\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Utilities\n",
    "# ------------------------------\n",
    "def atmosphere(h):\n",
    "    if h < 11000.0:\n",
    "        T = 288.15 - 0.0065 * h\n",
    "        p = 101325.0 * (T / 288.15) ** 5.255877\n",
    "    elif h < 20000.0:\n",
    "        T = 216.65\n",
    "        p11 = 101325.0 * (216.65 / 288.15) ** 5.255877\n",
    "        p = p11 * np.exp(- (h - 11000.0) / 6341.97)\n",
    "    else:\n",
    "        T = 216.65\n",
    "        rho0 = 0.08803\n",
    "        scale_h = 7000.0\n",
    "        rho = rho0 * np.exp(-(h - 20000.0)/scale_h)\n",
    "        p = rho * R * T\n",
    "        return T, p, rho\n",
    "    rho = p / (R * T)\n",
    "    return T, p, rho\n",
    "\n",
    "def viscosity_sutherland(T):\n",
    "    T = max(T, 1e-6)\n",
    "    return mu0 * (T / T0_ref)**1.5 * (T0_ref + S_sutherland) / (T + S_sutherland)\n",
    "\n",
    "# ------------------------------\n",
    "# Model builder (architecture unchanged)\n",
    "# ------------------------------\n",
    "def build_pinn_model(input_dim, shared_units=[128, 128], weight_decay=1e-6):\n",
    "    X_in = layers.Input(shape=(input_dim,), name=\"input_layer\")\n",
    "    shared = X_in\n",
    "    for i, u in enumerate(shared_units):\n",
    "        shared = layers.Dense(\n",
    "            u,\n",
    "            activation=\"tanh\",\n",
    "            kernel_regularizer=regularizers.l2(weight_decay),\n",
    "            name=f\"shared_dense_{i+1}\"\n",
    "        )(shared)\n",
    "    # Probability branch\n",
    "    prob = layers.Dense(64, activation=\"tanh\", name=\"prob_dense_1\")(shared)\n",
    "    prob = layers.Dense(1, activation=\"sigmoid\", name=\"probability\")(prob)\n",
    "    # Theta branch\n",
    "    theta_h = layers.Dense(64, activation=\"tanh\", name=\"theta_dense_1\")(shared)\n",
    "    theta_out = layers.Dense(1, activation=\"relu\", name=\"theta_pred\")(theta_h)\n",
    "    # Delta branch\n",
    "    delta_h = layers.Dense(32, activation=\"tanh\", name=\"delta_dense_1\")(shared)\n",
    "    delta_out = layers.Dense(1, activation=\"relu\", name=\"delta_pred\")(delta_h)\n",
    "    model = Model(inputs=X_in, outputs=[prob, theta_out, delta_out], name=\"PINN_Model\")\n",
    "    return model\n",
    "\n",
    "# ------------------------------\n",
    "# Custom physics-regularized losses (unchanged)\n",
    "# ------------------------------\n",
    "def build_custom_theta_loss(lambda_phys=LAMBDA_PHYS):\n",
    "    def theta_loss(y_true, y_pred):\n",
    "        theta_data = tf.expand_dims(y_true[:, 0], axis=-1)\n",
    "        theta_phys = tf.expand_dims(y_true[:, 1], axis=-1)\n",
    "        mse_data = tf.reduce_mean(tf.square(theta_data - y_pred))\n",
    "        mse_phys = tf.reduce_mean(tf.square(theta_phys - y_pred))\n",
    "        return mse_data + lambda_phys * mse_phys\n",
    "    return theta_loss\n",
    "\n",
    "def build_custom_delta_loss(lambda_phys=LAMBDA_PHYS):\n",
    "    def delta_loss(y_true, y_pred):\n",
    "        delta_data = tf.expand_dims(y_true[:, 0], axis=-1)\n",
    "        delta_phys = tf.expand_dims(y_true[:, 1], axis=-1)\n",
    "        mse_data = tf.reduce_mean(tf.square(delta_data - y_pred))\n",
    "        mse_phys = tf.reduce_mean(tf.square(delta_phys - y_pred))\n",
    "        return mse_data + lambda_phys * mse_phys\n",
    "    return delta_loss\n",
    "\n",
    "# ------------------------------\n",
    "# Upsampling (kept)\n",
    "# ------------------------------\n",
    "def upsample_minority(X, y_prob, y_theta_with_phys, y_delta_with_phys):\n",
    "    classes, counts = np.unique(y_prob.ravel(), return_counts=True)\n",
    "    if len(classes) == 1:\n",
    "        return X, y_prob, y_theta_with_phys, y_delta_with_phys\n",
    "    idx0 = np.where(y_prob.ravel() == 0)[0]\n",
    "    idx1 = np.where(y_prob.ravel() == 1)[0]\n",
    "    if len(idx1) > len(idx0):\n",
    "        idx0, idx1 = idx1, idx0\n",
    "    n0 = len(idx0); n1 = len(idx1)\n",
    "    if n1 == 0:\n",
    "        return X, y_prob, y_theta_with_phys, y_delta_with_phys\n",
    "    times = int(np.ceil(n0 / n1))\n",
    "    idx1_upsampled = np.tile(idx1, times)[:n0]\n",
    "    idx_balanced = np.concatenate([idx0, idx1_upsampled])\n",
    "    np.random.shuffle(idx_balanced)\n",
    "    return X[idx_balanced], y_prob[idx_balanced], y_theta_with_phys[idx_balanced], y_delta_with_phys[idx_balanced]\n",
    "\n",
    "# ------------------------------\n",
    "# Main pipeline\n",
    "# ------------------------------\n",
    "def main():\n",
    "    print(\"Loading dataset:\", CSV_PATH)\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "    # Verify required columns exist (features and targets)\n",
    "    missing_features = [c for c in FEATURE_COLS if c not in df.columns]\n",
    "    if missing_features:\n",
    "        print(\"Warning: some FEATURE_COLS missing from CSV. Those will be imputed or computed if possible:\", missing_features)\n",
    "\n",
    "    # Check targets\n",
    "    for target in [TARGET_COL_TRANS, TARGET_COL_THETA, TARGET_COL_DELTA]:\n",
    "        if target not in df.columns:\n",
    "            raise ValueError(f\"Missing required target/physics column in CSV: {target} (expected). Please check your CSV.\")\n",
    "\n",
    "    # If physics columns not present separately, use the same computed columns as physics values\n",
    "    phys_theta_col = PHYS_THETA_COL if PHYS_THETA_COL in df.columns else TARGET_COL_THETA\n",
    "    phys_delta_col = PHYS_DELTA_COL if PHYS_DELTA_COL in df.columns else TARGET_COL_DELTA\n",
    "\n",
    "    # Prepare features matrix (use FEATURE_COLS order)\n",
    "    X_df = df.copy()\n",
    "    for f in FEATURE_COLS:\n",
    "        if f not in X_df.columns:\n",
    "            # create column filled with NaN so imputer can fill\n",
    "            X_df[f] = np.nan\n",
    "\n",
    "    X_use = X_df[FEATURE_COLS].astype(float)\n",
    "\n",
    "    # Impute missing with median\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_imputed = imputer.fit_transform(X_use)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(\"Saved scaler to\", SCALER_PATH)\n",
    "\n",
    "    # Prepare packed targets: [data_target, physics_value]\n",
    "    y_prob_raw = df[TARGET_COL_TRANS].astype(float).values.reshape(-1, 1)\n",
    "    y_theta_packed = np.vstack([df[TARGET_COL_THETA].astype(float).values, df[phys_theta_col].astype(float).values]).T\n",
    "    y_delta_packed = np.vstack([df[TARGET_COL_DELTA].astype(float).values, df[phys_delta_col].astype(float).values]).T\n",
    "\n",
    "    # Train/test split with stratify on transition label\n",
    "    X_train, X_test, y_prob_train, y_prob_test, y_theta_train, y_theta_test, y_delta_train, y_delta_test = train_test_split(\n",
    "        X_scaled, y_prob_raw, y_theta_packed, y_delta_packed, test_size=TEST_SIZE, random_state=SEED, stratify=y_prob_raw\n",
    "    )\n",
    "\n",
    "    # Upsample minority\n",
    "    X_train_bal, y_prob_train_bal, y_theta_train_bal, y_delta_train_bal = upsample_minority(\n",
    "        X_train, y_prob_train, y_theta_train, y_delta_train\n",
    "    )\n",
    "\n",
    "    print(\"Train class distribution (after balancing):\", Counter(y_prob_train_bal.ravel()))\n",
    "    print(\"Test class distribution:\", Counter(y_prob_test.ravel()))\n",
    "\n",
    "    # Build model (same architecture)\n",
    "    model = build_pinn_model(input_dim=X_train_bal.shape[1], shared_units=[128, 128], weight_decay=1e-6)\n",
    "    model.summary()\n",
    "\n",
    "    # Losses & compile\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    theta_loss = build_custom_theta_loss(lambda_phys=LAMBDA_PHYS)\n",
    "    delta_loss = build_custom_delta_loss(lambda_phys=LAMBDA_PHYS)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=[bce_loss, theta_loss, delta_loss],\n",
    "        loss_weights=LOSS_WEIGHTS,\n",
    "        metrics={'probability': ['AUC']}\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    cb_early = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "    cb_chkpt = ModelCheckpoint(filepath=os.path.join(MODEL_DIR, \"best_pinn_model.h5\"), save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
    "    cb_reduce = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-7, verbose=1)\n",
    "\n",
    "    # Fit\n",
    "    history = model.fit(\n",
    "        X_train_bal,\n",
    "        [y_prob_train_bal, y_theta_train_bal, y_delta_train_bal],\n",
    "        validation_split=VAL_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[cb_early, cb_chkpt, cb_reduce],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Save final model\n",
    "    model.save(MODEL_PATH)\n",
    "    print(\"Saved trained model to\", MODEL_PATH)\n",
    "\n",
    "    # Evaluate on test\n",
    "    preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "    prob_pred = preds[0].ravel()\n",
    "    theta_pred = preds[1].ravel()\n",
    "    delta_pred = preds[2].ravel()\n",
    "\n",
    "    prob_true = y_prob_test.ravel()\n",
    "    theta_true = y_theta_test[:, 0]\n",
    "    theta_phys_test = y_theta_test[:, 1]\n",
    "    delta_true = y_delta_test[:, 0]\n",
    "    delta_phys_test = y_delta_test[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    try:\n",
    "        auc = roc_auc_score(prob_true, prob_pred)\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "    acc = accuracy_score(prob_true, (prob_pred > 0.5).astype(int))\n",
    "    theta_mse = mean_squared_error(theta_true, theta_pred)\n",
    "    theta_mae = mean_absolute_error(theta_true, theta_pred)\n",
    "    delta_mse = mean_squared_error(delta_true, delta_pred)\n",
    "    delta_mae = mean_absolute_error(delta_true, delta_pred)\n",
    "    theta_phys_dev = mean_squared_error(theta_phys_test, theta_pred)\n",
    "    delta_phys_dev = mean_squared_error(delta_phys_test, delta_pred)\n",
    "\n",
    "    print(\"\\n--- TEST METRICS ---\")\n",
    "    print(\"AUC (prob):\", auc)\n",
    "    print(\"Accuracy (prob @0.5):\", acc)\n",
    "    print(\"Theta -> MSE: {:.6e}, MAE: {:.6e}\".format(theta_mse, theta_mae))\n",
    "    print(\"Delta -> MSE: {:.6e}, MAE: {:.6e}\".format(delta_mse, delta_mae))\n",
    "    print(\"Theta physics dev (MSE vs physics): {:.6e}\".format(theta_phys_dev))\n",
    "    print(\"Delta physics dev (MSE vs physics): {:.6e}\".format(delta_phys_dev))\n",
    "\n",
    "    # Save test predictions in a CSV with column names matching RF pipeline pattern\n",
    "    out_df = pd.DataFrame({\n",
    "        \"transition_true\": prob_true,\n",
    "        \"transition_pred\": prob_pred,\n",
    "        \"momentum_theta_true\": theta_true,\n",
    "        \"momentum_theta_pred\": theta_pred,\n",
    "        \"momentum_theta_blasius\": theta_phys_test,\n",
    "        \"delta_99_true\": delta_true,\n",
    "        \"delta_99_pred\": delta_pred,\n",
    "        \"delta_99_blasius\": delta_phys_test\n",
    "    })\n",
    "    out_df.to_csv(OUT_PRED_CSV, index=False)\n",
    "    print(\"Saved test predictions to\", OUT_PRED_CSV)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc8edf-cd43-4e54-8a1b-cb5a5ddce28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
